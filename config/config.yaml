defaults:
  - _self_

  - model: null
  - data: ???
  - visualization: null

  - loss: default_loss
  - metrics: default_metrics

experiment:
  project: cross_view_transformers_test

  # wandb project
  #新训练
  uuid: ${now:%m%d_%H%M%S}
  #uuid: '0220_232747'
  save_dir: ${hydra:runtime.cwd}/logs/                # log directory, will be created

  seed: 2022
  checkpoint_interval: 1000
  log_image_interval: 500

  #lhy modify  true 是继续训练  false 是从新训练
  resume: true

#lhy modify  训练数据集路径
loader:
  batch_size: 2
  num_workers: 2
  pin_memory: True
  prefetch_factor: 2

# AdamW
optimizer:
  lr: 1e-3
  weight_decay: 1e-7

# OneCycleLR
scheduler:
  div_factor: 10                                      # starts at lr / 10
  pct_start: 0.3                                      # reaches lr at 30% of total steps
  final_div_factor: 10                                # ends at lr / 10 / 10
  max_lr: ${optimizer.lr}
  total_steps: ${trainer.max_steps}
  cycle_momentum: False

# lightning trainer
trainer:
  max_steps: 30001
  log_every_n_steps: 50

  accelerator: gpu
  devices: auto
  precision: 16-mixed

  accumulate_grad_batches: 2  # 每 2 个 mini-batch 梯度累积一次，相当于整体 batch size 为 2*2 = 4
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  check_val_every_n_epoch: 1
  val_check_interval: 1.0
  num_sanity_val_steps: 0
  gradient_clip_val: 5.0
  sync_batchnorm: False
